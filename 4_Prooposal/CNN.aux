\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{british}{}
\citation{lecun1989generalization}
\citation{goodfellow2016deep,deeppython}
\citation{goodfellow2016deep}
\citation{freund1999large}
\citation{deeppython}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Scheme of a single perceptron. Reference: \url  {http://www.techmaru.com/technology/artificial-neural-networks/neural-network-elements}.\relax }}{1}{figure.caption.3}}
\citation{bracewell1986fourier}
\citation{hirschman2012convolution}
\citation{goodfellow2016deep}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Scheme of a Multilayer perceptron. Reference: \url  {https://www.researchgate.net/figure/A-hypothetical-example-of-Multilayer-Perceptron-Network_fig2_273768094}.\relax }}{2}{figure.caption.4}}
\citation{goodfellow2016deep}
\citation{papoulis1962fourier}
\citation{goodfellow2016deep}
\citation{cs231n}
\citation{cs231n}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\citation{bhandare2016applications}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textit  {Sparse connectivity}: We highlight one output unit, $s_3$, and also highlight the input units in $x$ that affect this unit. These units are known as the receptive field of $s_3$. (Top) When $s$ is formed by convolution with a kernel of width 3, only three inputs affect $s_3$. (Bottom) When $s$ is formed by matrix multiplication, connectivity is no longer sparse, so all of the inputs affect $s_3$. Reference: \cite  {goodfellow2016deep}.\relax }}{4}{figure.caption.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textit  {Parameter sharing}: Black arrows indicate the connections that use a particular parameter in two different models. (Top) The black arrows indicate uses of the central element of a 3-element kernel in a convolutional model. Due to parameter sharing, this single parameter is used at all input locations. (Bottom) The single black arrow indicates the use of the central element of the weight matrix in a fully connected model. This model has no parameter sharing so the parameter is used only once. Reference: \cite  {goodfellow2016deep}.\relax }}{4}{figure.caption.8}}
\citation{colah}
\citation{jarrett2009best,glorot2011deep,nair2010rectified}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Example of a 2D convolution using a $3\times 3$ kernel. Since we are not including \textit  {zero-padding}, the dimension of the feature map is smaller than the input. Reference: \url  {https://github.com/PetarV-/TikZ/tree/master/2D\%20Convolution}.\relax }}{6}{figure.caption.10}}
\citation{zhou1988computation}
\citation{nasrabadi2007pattern}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Non-linearity layer performance by using ReLU as activation function. Those numbers that are negative become zero in the feature map. Reference: \url  {https://medium.com/data-science-group-iitr/building-a-convolutional-neural-network-in-python-with-tensorflow-d251c3ca8117}.\relax }}{7}{figure.caption.11}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Pooling layer implementation using max pooling operator (\textit  {Top}) and average pooling (\textit  {Bottom}) in a square neighborhood. Reference: \url  {https://medium.com/data-science-group-iitr/building-a-convolutional-neural-network-in-python-with-tensorflow-d251c3ca8117}.\relax }}{7}{figure.caption.12}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Architecture of a fully-connected neural network. Reference: \url  {https://hackernoon.com/deep-learning-cnns-in-tensorflow-with-gpus-cba6efe0acc2/}.\relax }}{8}{figure.caption.13}}
\bibstyle{apalike}
\bibdata{refs_cnn}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Architecture of a typical convolutional neural network. non-linearity layer are usually included between the convolutional layer and the pooling layer. Reference: \url  {https://www.pyimagesearch.com/2014/06/09/get-deep-learning-bandwagon-get-perspective/}.\relax }}{9}{figure.caption.14}}
